{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Machine Learning Engineer ‚Äì LLM Task Assignment**\n",
        "###**Company:** Artikate Studio, Artikate Private Limited\n",
        "###**Task Title:** LLM-Powered Fact Checker with Custom Embedding-Based Retrieval\n",
        "###**Submission Deadline:** 24 Navember, 2025 7:00 pm\n",
        "###**Candidate name:** Mitul Srivastava\n"
      ],
      "metadata": {
        "id": "0Se06bPvEzv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Summary**\n",
        "\n",
        "This notebook builds a complete automated fact-checking pipeline powered by:\n",
        "\n",
        "* RSS Scraping (Press Information Bureau of India)\n",
        "\n",
        "* Text Cleaning & Chunking\n",
        "\n",
        "* Sentence Embeddings (SentenceTransformers)\n",
        "\n",
        "* FAISS Vector Search\n",
        "\n",
        "* spaCy-based Claim Extraction\n",
        "\n",
        "* Groq Llama 70B for Final Verdict\n",
        "\n",
        "* Gradio UI for interactive fact checking\n",
        "\n",
        "üîç How It Works (High-Level Workflow)\n",
        "\n",
        "1. Data Collection\n",
        "Scrapes official PIB RSS feeds ‚Üí extracts titles ‚Üí stores them as trusted factual statements.\n",
        "\n",
        "2. Fact Preparation\n",
        "Cleans, normalizes, chunks statements ‚Üí encodes them into embeddings ‚Üí builds a FAISS index for fast retrieval.\n",
        "\n",
        "3. Claim Extraction\n",
        "Given a user query, spaCy extracts the main actionable claim.\n",
        "\n",
        "4. Similarity Search (FAISS)\n",
        "Retrieves the most relevant government facts based on embedding similarity.\n",
        "\n",
        "5. LLM Verification (Groq + Llama 70B)\n",
        "The model receives:\n",
        "\n",
        "* The claim\n",
        "* The retrieved factual evidence\n",
        "And returns a structured JSON verdict: True / False / Unverifiable with reasoning.\n",
        "\n",
        "6. Final User Output\n",
        "Nicely formatted result with:\n",
        "\n",
        "* Verdict\n",
        "* Confidence\n",
        "* Reasoning\n",
        "* Top evidence snippets\n",
        "\n",
        "7. Gradio App\n",
        "A simple UI lets anyone enter claims and instantly verify them."
      ],
      "metadata": {
        "id": "P8XxmV7muzaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Install Dependencies**"
      ],
      "metadata": {
        "id": "w85pJiC1vPX6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UzCL_z1eul2n"
      },
      "outputs": [],
      "source": [
        "# Install required Python libraries for the project\n",
        "# - sentence-transformers: For generating sentence embeddings\n",
        "# - faiss-cpu: For efficient vector search\n",
        "# - spacy + en_core_web_md: For NLP preprocessing and word vectors\n",
        "# - gradio: For building an interactive UI\n",
        "# - pandas: For data manipulation\n",
        "# - groq: For Groq API integration (LLM inference)\n",
        "# - huggingface_hub: For downloading models/data from Hugging Face\n",
        "# - feedparser, beautifulsoup4, requests, lxml: For web scraping & RSS parsing\n",
        "\n",
        "!pip install -q sentence-transformers faiss-cpu spacy gradio pandas groq huggingface_hub feedparser beautifulsoup4 requests lxml\n",
        "\n",
        "# Download the medium-sized English model for spaCy\n",
        "!python -m spacy download en_core_web_md\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Import Libraries**"
      ],
      "metadata": {
        "id": "PUrBXph3voWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import json\n",
        "from typing import List, Dict, Tuple\n",
        "from datetime import datetime\n",
        "from urllib.parse import urljoin\n",
        "import re\n",
        "import time\n",
        "import textwrap\n",
        "\n",
        "# Data handling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# NLP and Embeddings\n",
        "import spacy\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Vector search\n",
        "import faiss\n",
        "\n",
        "# LLM / API clients\n",
        "from groq import Groq\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Web parsing & requests\n",
        "import requests\n",
        "import feedparser\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully\")\n"
      ],
      "metadata": {
        "id": "Z5ZKCz9ovt60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Configure API Keys & Authentication**"
      ],
      "metadata": {
        "id": "FCTIz_B4wgS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('HF_TOKEN')\n",
        "userdata.get('GROQ_API_KEY')"
      ],
      "metadata": {
        "id": "0mE1IyauFmLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------\n",
        "# Hugging Face Authentication\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "# IMPORTANT:\n",
        "# Do NOT hardcode private tokens in notebooks.\n",
        "# Use environment variables or secrets instead.\n",
        "# Replace \"YOUR_HF_TOKEN_HERE\" with your real token securely.\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\", \"YOUR_HF_TOKEN_HERE\")\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "os.environ[\"HUGGINGFACE_TOKEN\"] = HF_TOKEN\n",
        "\n",
        "print(\"‚úÖ Hugging Face token set (environment-based)\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Groq API Authentication\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "# Securely fetch API key from environment or Colab userdata\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\", \"YOUR_GROQ_API_KEY_HERE\")\n",
        "\n",
        "# Google Colab secret support (fallback method)\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    GROQ_API_KEY = userdata.get(\"GROQ_API_KEY\") or GROQ_API_KEY\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# Initialize Groq client\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "print(\"‚úÖ Groq API configured successfully\")\n"
      ],
      "metadata": {
        "id": "XCihh0P9wlFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. RSS Fetching Function**"
      ],
      "metadata": {
        "id": "V8e_jqhuxSg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_pib_rss():\n",
        "    \"\"\"\n",
        "    Fetch RSS entries from multiple PIB (Press Information Bureau) RSS feeds.\n",
        "\n",
        "    Returns:\n",
        "        list: A combined list of RSS feed entries from all specified PIB categories.\n",
        "    \"\"\"\n",
        "\n",
        "    # PIB RSS feed URLs (categorized by domain)\n",
        "    urls = [\n",
        "        \"https://www.pib.gov.in/RssMain.aspx?ModId=6&Lang=1&Regid=3\",  # Press Releases\n",
        "        \"https://www.pib.gov.in/RssMain.aspx?ModId=6&Lang=1&Regid=1\",  # Finance\n",
        "        \"https://www.pib.gov.in/RssMain.aspx?ModId=6&Lang=1&Regid=2\",  # Cabinet\n",
        "        \"https://www.pib.gov.in/RssMain.aspx?ModId=6&Lang=1&Regid=4\",  # Health\n",
        "        \"https://www.pib.gov.in/RssMain.aspx?ModId=6&Lang=1&Regid=7\",  # Education\n",
        "        \"https://www.pib.gov.in/RssMain.aspx?ModId=6&Lang=1&Regid=9\",  # Environment\n",
        "    ]\n",
        "\n",
        "    # Custom headers to avoid request blocking by the PIB server\n",
        "    headers = {\n",
        "        \"User-Agent\": (\n",
        "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "            \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36\"\n",
        "        ),\n",
        "        \"Accept\": \"application/xml,text/xml,application/xhtml+xml,text/html\",\n",
        "        \"Referer\": \"https://pib.gov.in/\",\n",
        "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "    }\n",
        "\n",
        "    all_entries = []\n",
        "\n",
        "    # Iterate through all RSS feed URLs\n",
        "    for url in urls:\n",
        "        try:\n",
        "            print(f\"\\nüîé Fetching RSS: {url}\")\n",
        "            response = requests.get(url, headers=headers, timeout=10)\n",
        "            print(\"   ‚Ü≥ Status Code:\", response.status_code)\n",
        "\n",
        "            # Parse RSS content\n",
        "            feed = feedparser.parse(response.text)\n",
        "\n",
        "            if feed.entries:\n",
        "                print(f\"   ‚úì Found {len(feed.entries)} entries\")\n",
        "                all_entries.extend(feed.entries)\n",
        "            else:\n",
        "                print(\"   ‚ö† No entries found in this feed\")\n",
        "\n",
        "        except requests.exceptions.Timeout:\n",
        "            print(f\"   ‚ùå Timeout error for URL: {url}\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"   ‚ùå Request failed for {url}: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Unexpected error: {e}\")\n",
        "\n",
        "    return all_entries\n"
      ],
      "metadata": {
        "id": "zCXooIj9U8oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_facts = [\n",
        "    \"The Indian government launched PM-KISAN scheme providing ‚Çπ6000 annual income support to farmer families in December 2018.\",\n",
        "    \"Goods and Services Tax (GST) was implemented in India on July 1, 2017.\",\n",
        "    \"India achieved 100 crore COVID-19 vaccinations on October 21, 2021.\",\n",
        "    \"The National Education Policy (NEP) 2020 was approved by the Union Cabinet on July 29, 2020.\",\n",
        "    \"Ayushman Bharat scheme provides health insurance coverage up to ‚Çπ5 lakh per family per year.\",\n",
        "    \"India's unemployment rate was 7.8% in November 2024 according to CMIE data.\",\n",
        "    \"The Reserve Bank of India kept the repo rate unchanged at 6.5% in December 2024.\",\n",
        "    \"India's GDP growth rate for Q2 FY 2024-25 was 6.7% according to NSO data.\",\n",
        "    \"The Pradhan Mantri Awas Yojana aims to provide housing for all by 2024.\",\n",
        "    \"India launched Chandrayaan-3 successfully on July 14, 2023.\",\n",
        "    \"The Indian government announced production-linked incentive scheme for semiconductor manufacturing in December 2021.\",\n",
        "    \"Aadhaar has over 134 crore enrollments as of September 2024.\",\n",
        "    \"The Swachh Bharat Mission achieved 100% village ODF status in October 2019.\",\n",
        "    \"India's foreign exchange reserves stood at $622 billion as of November 2024.\",\n",
        "    \"The government launched ONDC (Open Network for Digital Commerce) in April 2022.\",\n",
        "    \"India's renewable energy capacity reached 180 GW in November 2024.\",\n",
        "    \"The National Hydrogen Mission was launched in August 2021.\",\n",
        "    \"PM Fasal Bima Yojana provides crop insurance to farmers with subsidized premiums.\",\n",
        "    \"India's EV sales grew by 50% in 2024 compared to 2023.\",\n",
        "    \"The government reduced corporate tax rate to 22% for domestic companies in September 2019.\",\n",
        "    \"Digital transactions in India crossed 13,462 crore in value during FY 2023-24.\",\n",
        "    \"India's edible oil imports were 165 lakh tonnes in FY 2023-24.\",\n",
        "    \"The National Rail Plan aims to create a future-ready railway system by 2030.\",\n",
        "    \"India's per capita income increased to ‚Çπ1,72,000 in FY 2023-24.\",\n",
        "    \"The government launched PM SVANidhi scheme for street vendors in June 2020.\",\n",
        "    \"India exported pharmaceuticals worth $27.9 billion in FY 2023-24.\",\n",
        "    \"The Production Linked Incentive (PLI) scheme covers 14 sectors.\",\n",
        "    \"India's solar power capacity reached 81 GW as of October 2024.\",\n",
        "    \"The Smart Cities Mission was launched in June 2015 covering 100 cities.\",\n",
        "    \"India's startup ecosystem is the third-largest globally.\",\n",
        "    \"The government extended free food grain scheme (PMGKAY) till December 2024.\",\n",
        "    \"India's merchandise exports reached $437 billion in FY 2023-24.\",\n",
        "    \"The National Logistics Policy was launched in September 2022.\",\n",
        "    \"India's installed power generation capacity reached 442 GW in November 2024.\",\n",
        "    \"The government launched e-Shram portal for unorganized workers in August 2021.\",\n",
        "    \"India's automobile production was 255 lakh vehicles in FY 2023-24.\",\n",
        "    \"The PM-KUSUM scheme promotes solar pumps for farmers.\",\n",
        "    \"India's services sector accounts for 55% of GDP as of 2024.\",\n",
        "    \"The government approved National Green Hydrogen Mission with ‚Çπ19,744 crore outlay.\",\n",
        "    \"India's internet users crossed 90 crore in 2024.\",\n",
        "    \"The National Infrastructure Pipeline envisages ‚Çπ111 lakh crore investment by 2025.\",\n",
        "    \"India's defense budget for FY 2024-25 is ‚Çπ6.21 lakh crore.\",\n",
        "    \"The government launched Skill India Digital platform in February 2023.\",\n",
        "    \"India's coal production reached 997 million tonnes in FY 2023-24.\",\n",
        "    \"The PM Vishwakarma scheme provides support to traditional artisans.\",\n",
        "    \"India's crude oil production was 29.7 million tonnes in FY 2023-24.\",\n",
        "    \"The National Medical Commission replaced Medical Council of India in September 2020.\",\n",
        "    \"India's diamond exports were $23.7 billion in FY 2023-24.\",\n",
        "    \"The government launched Atal Innovation Mission to promote innovation.\",\n",
        "    \"India's urban population is expected to reach 60 crore by 2031.\"\n",
        "]"
      ],
      "metadata": {
        "id": "e5w429fS51Q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. RSS Scraping & Fact Extraction**"
      ],
      "metadata": {
        "id": "7XDFWuAeRTvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_pib_rss(num_facts=100):\n",
        "    \"\"\"\n",
        "    Extracts titles and links from PIB RSS feeds,\n",
        "    and appends manually created sample facts.\n",
        "\n",
        "    Args:\n",
        "        num_facts (int): Maximum number of facts to return\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[str], List[str]]:\n",
        "            - List of fact titles\n",
        "            - List of corresponding links\n",
        "    \"\"\"\n",
        "\n",
        "    all_facts = []\n",
        "    all_links = []\n",
        "\n",
        "    # 1. Fetch entries from all PIB RSS feeds\n",
        "    entries = fetch_pib_rss()\n",
        "\n",
        "    # 2. Extract title and link from each RSS entry\n",
        "    for entry in entries:\n",
        "        try:\n",
        "            title = entry.title.strip()\n",
        "            link = entry.link.strip()\n",
        "        except:\n",
        "            # Skip entries missing expected fields\n",
        "            continue\n",
        "\n",
        "        all_facts.append(title)\n",
        "        all_links.append(link)\n",
        "\n",
        "    # 3. Append manually created sample facts (must exist in notebook)\n",
        "    try:\n",
        "        for fact in sample_facts:\n",
        "            all_facts.append(fact)\n",
        "            all_links.append(\"N/A\")\n",
        "    except NameError:\n",
        "        print(\"‚ö† sample_facts is not defined. Skipping manual facts.\")\n",
        "\n",
        "    # 4. Return limited number of results\n",
        "    return all_facts[:num_facts], all_links[:num_facts]\n"
      ],
      "metadata": {
        "id": "mJW_XeKS6Foq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Create DataFrame**"
      ],
      "metadata": {
        "id": "W_DKtEz1xeuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Cell 6 ‚Äî Convert Facts to DataFrame\n",
        "# ==========================================\n",
        "\n",
        "facts, links = scrape_pib_rss(num_facts=100)\n",
        "\n",
        "df_facts = pd.DataFrame({\n",
        "    \"id\": range(len(facts)),\n",
        "    \"statement\": facts,\n",
        "    \"source\": \"PIB/Government\",\n",
        "    \"date_added\": datetime.now().strftime(\"%Y-%m-%d\")\n",
        "})\n",
        "\n",
        "df_facts.head()\n"
      ],
      "metadata": {
        "id": "5vD1phPfxkyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7. Save & Reload Dataset**"
      ],
      "metadata": {
        "id": "yzB3DPQ4RfH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------\n",
        "# Save extracted facts to CSV & reload to verify\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "csv_filename = 'verified_facts_database.csv'\n",
        "df_facts.to_csv(csv_filename, index=False, encoding='utf-8')\n",
        "\n",
        "print(\"\\nüìä Dataset Statistics:\")\n",
        "print(\"   Total records saved:\", len(df_facts))\n",
        "\n",
        "# Verify by reloading\n",
        "df_loaded = pd.read_csv(csv_filename)\n",
        "print(f\"‚úÖ Loaded {len(df_loaded)} facts from CSV\")\n"
      ],
      "metadata": {
        "id": "JXjFRP1MYsSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8. Embedding + FAISS Indexing**"
      ],
      "metadata": {
        "id": "ePxJ8BKCRko8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1. Load Sentence Transformer embedding model\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# NOTE:\n",
        "# The newer SentenceTransformer versions no longer use `use_auth_token`\n",
        "# If HF token is required, use: login(os.environ[\"HF_TOKEN\"])\n",
        "embedding_model = SentenceTransformer(\n",
        "    \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Loaded embedding model\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2. Text Chunking Utility\n",
        "# ------------------------------------------------------------\n",
        "def chunk_text(text, max_len=300):\n",
        "    \"\"\"\n",
        "    Split text into smaller chunks to ensure better semantic embedding.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input full text.\n",
        "        max_len (int): Maximum chunk length.\n",
        "\n",
        "    Returns:\n",
        "        list[str]: List of chunked strings.\n",
        "    \"\"\"\n",
        "    text = text.replace(\"\\n\", \" \").strip()\n",
        "    return textwrap.wrap(text, max_len)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3. Extract & Chunk Text From DataFrame\n",
        "# ------------------------------------------------------------\n",
        "fact_texts = []\n",
        "for statement in df_loaded[\"statement\"].tolist():\n",
        "    chunks = chunk_text(statement)\n",
        "    fact_texts.extend(chunks)\n",
        "\n",
        "print(f\"üìÑ Total chunks after splitting: {len(fact_texts)}\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4. Create Embeddings\n",
        "# ------------------------------------------------------------\n",
        "print(\"üîÑ Encoding chunks into embeddings...\")\n",
        "embeddings = embedding_model.encode(\n",
        "    fact_texts,\n",
        "    show_progress_bar=True\n",
        ")\n",
        "\n",
        "embeddings = np.array(embeddings).astype(\"float32\")\n",
        "print(f\"üß† Embedding shape: {embeddings.shape}\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5. Build FAISS Index\n",
        "# ------------------------------------------------------------\n",
        "dimension = embeddings.shape[1]\n",
        "faiss_index = faiss.IndexFlatL2(dimension)\n",
        "faiss_index.add(embeddings)\n",
        "\n",
        "print(f\"‚úÖ FAISS index created with {faiss_index.ntotal} vectors of {dimension} dims\")\n"
      ],
      "metadata": {
        "id": "jO3orhApyUH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **9. Relevant Fact Retrieval**"
      ],
      "metadata": {
        "id": "cX6B6Fn3Rtkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------\n",
        "# Retrieve Top-K Relevant Facts Using FAISS Vector Search\n",
        "# --------------------------------------------------------\n",
        "def retrieve_relevant_facts(query: str, top_k: int = 5):\n",
        "    \"\"\"\n",
        "    Encodes a user query into an embedding, searches the FAISS index,\n",
        "    and returns the top-k most similar fact chunks.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    query : str\n",
        "        The user query / claim to match against the knowledge base.\n",
        "    top_k : int\n",
        "        Number of top relevant facts to return.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List[Tuple[str, float]]\n",
        "        A list of (fact_text, similarity_score).\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Encode the input query using the same embedding model\n",
        "    query_embedding = embedding_model.encode([query]).astype(\"float32\")\n",
        "\n",
        "    # 2. Perform similarity search using FAISS (L2 distance)\n",
        "    distances, indices = faiss_index.search(query_embedding, top_k)\n",
        "\n",
        "    results = []\n",
        "    for idx, dist in zip(indices[0], distances[0]):\n",
        "        # Ensure returned index is in range\n",
        "        if idx < len(fact_texts):\n",
        "            # Convert L2 distance to a similarity score (bounded)\n",
        "            similarity = 1 / (1 + dist)\n",
        "            results.append((fact_texts[idx], similarity))\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "print(\"‚úÖ Retrieval system ready\")\n"
      ],
      "metadata": {
        "id": "fkpjBwMn0ec0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **10. Claim Extraction (spaCy)**"
      ],
      "metadata": {
        "id": "WYWwnlmFRxsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy medium English model\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "\n",
        "def extract_claims(text: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Extracts meaningful claims from a given text.\n",
        "    For each sentence, the function returns:\n",
        "        - the sentence text\n",
        "        - named entities (grouped by type)\n",
        "        - key nouns, verbs, and proper nouns (lemmatized)\n",
        "    \"\"\"\n",
        "\n",
        "    doc = nlp(text)\n",
        "    claims = []\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        sent_doc = nlp(sent.text)\n",
        "\n",
        "        # Extract named entities\n",
        "        entities = {}\n",
        "        for ent in sent_doc.ents:\n",
        "            entities.setdefault(ent.label_, []).append(ent.text)\n",
        "\n",
        "        # Extract keywords (nouns, proper nouns, verbs)\n",
        "        keywords = [\n",
        "            token.lemma_\n",
        "            for token in sent_doc\n",
        "            if token.pos_ in {\"NOUN\", \"PROPN\", \"VERB\"} and not token.is_stop\n",
        "        ]\n",
        "\n",
        "        claims.append({\n",
        "            \"text\": sent.text.strip(),\n",
        "            \"entities\": entities,\n",
        "            \"keywords\": keywords\n",
        "        })\n",
        "\n",
        "    return claims\n"
      ],
      "metadata": {
        "id": "sFkXNcC8x4Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **11. LLM Verification**"
      ],
      "metadata": {
        "id": "r23UKJJM0fcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_claim_with_llm(claim: str, retrieved_facts: List[Tuple[str, float]]) -> Dict:\n",
        "    \"\"\"\n",
        "    Verify a claim using Groq's Llama models.\n",
        "    Returns a structured JSON verdict with reasoning.\n",
        "    \"\"\"\n",
        "\n",
        "    # Format retrieved evidence as numbered list\n",
        "    evidence_text = \"\\n\".join([\n",
        "        f\"{i+1}. {fact} (Relevance: {score:.2f})\"\n",
        "        for i, (fact, score) in enumerate(retrieved_facts)\n",
        "    ])\n",
        "\n",
        "    # Fact-checking prompt (strict JSON output)\n",
        "    prompt = f\"\"\"\n",
        "You are a strict fact-checking assistant. Evaluate the claim using ONLY the verified evidence provided.\n",
        "\n",
        "CLAIM:\n",
        "\"{claim}\"\n",
        "\n",
        "VERIFIED EVIDENCE:\n",
        "{evidence_text}\n",
        "\n",
        "TASK:\n",
        "Determine whether the claim is True, False, or Unverifiable using ONLY the verified evidence.\n",
        "Respond in **valid JSON only**, no natural language, no formatting:\n",
        "\n",
        "{{\n",
        "  \"verdict\": \"True\" | \"False\" | \"Unverifiable\",\n",
        "  \"confidence\": float between 0 and 1,\n",
        "  \"reasoning\": \"Short explanation\",\n",
        "  \"evidence_used\": [list of evidence numbers]\n",
        "}}\n",
        "\"\"\".strip()\n",
        "\n",
        "    try:\n",
        "        # Query Groq model\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llama-3.3-70b-versatile\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"Return only valid JSON. No prose, no markdown.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.1,\n",
        "            max_tokens=400\n",
        "        )\n",
        "\n",
        "        raw_output = response.choices[0].message.content.strip()\n",
        "\n",
        "        # Clean accidental markdown fences (common in LLM responses)\n",
        "        if raw_output.startswith(\"```\"):\n",
        "            raw_output = raw_output.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "\n",
        "        # Parse JSON output safely\n",
        "        result = json.loads(raw_output)\n",
        "\n",
        "        # Attach actual retrieved evidence text\n",
        "        result[\"evidence\"] = [fact for fact, _ in retrieved_facts]\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        # Fallback in case model outputs invalid or unparsable JSON\n",
        "        return {\n",
        "            \"verdict\": \"Unverifiable\",\n",
        "            \"confidence\": 0.0,\n",
        "            \"reasoning\": f\"Model error: {str(e)}\",\n",
        "            \"evidence\": [fact for fact, _ in retrieved_facts]\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ LLM verification ready (Groq + Llama 3.x)\")\n"
      ],
      "metadata": {
        "id": "HrWgQs6l0jml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **12. FactChecker Class**"
      ],
      "metadata": {
        "id": "9jqc6yC60pkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FactChecker:\n",
        "    \"\"\"\n",
        "    A class that performs claim extraction, retrieval of relevant facts,\n",
        "    and verification using an LLM (Groq Llama models).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Reuse loaded global models and indexes\n",
        "        self.nlp = nlp\n",
        "        self.embedding_model = embedding_model\n",
        "        self.faiss_index = faiss_index\n",
        "\n",
        "        # Load facts from the CSV-loaded dataframe\n",
        "        self.facts = df_loaded[\"statement\"].tolist()\n",
        "\n",
        "    def check_fact(self, input_text: str, top_k: int = 5) -> Dict:\n",
        "        \"\"\"\n",
        "        Extract the main claim from text, retrieve supporting evidence,\n",
        "        and verify the claim using the LLM.\n",
        "        \"\"\"\n",
        "\n",
        "        # Extract claims using NLP\n",
        "        claims = extract_claims(input_text)\n",
        "\n",
        "        # If multiple sentences, check the first claim (main claim heuristic)\n",
        "        main_claim = claims[0][\"text\"] if claims else input_text\n",
        "\n",
        "        # Retrieve relevant facts\n",
        "        retrieved_facts = retrieve_relevant_facts(main_claim, top_k)\n",
        "\n",
        "        # Verify the claim using LLM\n",
        "        result = verify_claim_with_llm(main_claim, retrieved_facts)\n",
        "\n",
        "        # Add useful metadata\n",
        "        result[\"input_text\"] = input_text\n",
        "        result[\"extracted_claim\"] = main_claim\n",
        "        result[\"timestamp\"] = datetime.now().isoformat()\n",
        "\n",
        "        return result\n",
        "\n",
        "    def format_output(self, result: Dict) -> str:\n",
        "        \"\"\"\n",
        "        Format the LLM verification output into a clean, readable message.\n",
        "        \"\"\"\n",
        "\n",
        "        verdict_emoji = {\n",
        "            \"True\": \"‚úÖ\",\n",
        "            \"False\": \"‚ùå\",\n",
        "            \"Unverifiable\": \"‚ùì\"\n",
        "        }\n",
        "\n",
        "        verdict = result.get(\"verdict\", \"Unverifiable\")\n",
        "        emoji = verdict_emoji.get(verdict, \"‚ùì\")\n",
        "\n",
        "        # Build formatted output\n",
        "        output = f\"\"\"\n",
        "{emoji} VERDICT: {verdict}\n",
        "Confidence: {result.get('confidence', 0):.0%}\n",
        "\n",
        "üìù REASONING:\n",
        "{result.get('reasoning', 'No reasoning provided')}\n",
        "\n",
        "üìö EVIDENCE REVIEWED:\n",
        "\"\"\"\n",
        "        # Limit displayed evidence to first 3 items\n",
        "        for i, evidence in enumerate(result.get(\"evidence\", [])[:3], 1):\n",
        "            output += f\"\\n{i}. {evidence}\"\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# Initialize fact-checker instance\n",
        "fact_checker = FactChecker()\n",
        "print(\"‚úÖ Fact Checker initialized\")\n"
      ],
      "metadata": {
        "id": "2RWAvj6E0s6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **13. Test Cases**"
      ],
      "metadata": {
        "id": "zNBP9QRq0yZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------\n",
        "# TESTING FACT CHECKER PIPELINE\n",
        "# ---------------------------------------------\n",
        "\n",
        "test_inputs = [\n",
        "    \"The Indian government has announced free electricity to all farmers starting July 2025.\",\n",
        "    \"India achieved 100 crore COVID-19 vaccinations in October 2021.\",\n",
        "    \"The moon is made of cheese according to NASA.\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TESTING FACT CHECKER\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for test_input in test_inputs:\n",
        "    print(f\"\\nINPUT: {test_input}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Run fact-checking pipeline\n",
        "    result = fact_checker.check_fact(test_input)\n",
        "\n",
        "    # Print formatted verdict\n",
        "    print(fact_checker.format_output(result))\n"
      ],
      "metadata": {
        "id": "RZDfV2Iq012h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **14. Gradio UI**"
      ],
      "metadata": {
        "id": "xyeA5kfI05O2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def check_fact_ui(input_text, num_sources):\n",
        "    \"\"\"\n",
        "    Wrapper for the Gradio UI.\n",
        "    Takes user input, performs fact checking, and returns formatted output.\n",
        "    \"\"\"\n",
        "    if not input_text.strip():\n",
        "        return \"‚ö†Ô∏è Please enter a claim to verify.\"\n",
        "\n",
        "    result = fact_checker.check_fact(input_text, top_k=num_sources)\n",
        "    return fact_checker.format_output(result)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Gradio UI\n",
        "# -----------------------------\n",
        "with gr.Blocks(\n",
        "    theme=gr.themes.Soft(),\n",
        "    title=\"LLM Fact Checker\"\n",
        ") as demo:\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    # üîç LLM-Powered Fact Checker\n",
        "    ### Verify claims against trusted government sources\n",
        "\n",
        "    **Tech Stack**\n",
        "    - **Claim Extraction:** spaCy\n",
        "    - **Embeddings:** all-MiniLM-L6-v2\n",
        "    - **Vector Search:** FAISS\n",
        "    - **LLM:** Groq API (Llama 3.x)\n",
        "\n",
        "    Enter a claim below and the system will extract the main claim, retrieve\n",
        "    relevant government-verified facts, and generate a structured verdict.\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "\n",
        "        # Input Section\n",
        "        with gr.Column(scale=2):\n",
        "            input_text = gr.Textbox(\n",
        "                label=\"Enter a claim\",\n",
        "                placeholder=(\n",
        "                    \"Example: The Indian government has announced free electricity \"\n",
        "                    \"to all farmers starting July 2025.\"\n",
        "                ),\n",
        "                lines=4\n",
        "            )\n",
        "\n",
        "            num_sources = gr.Slider(\n",
        "                minimum=3,\n",
        "                maximum=10,\n",
        "                value=5,\n",
        "                step=1,\n",
        "                label=\"Number of evidence sources to retrieve\"\n",
        "            )\n",
        "\n",
        "            check_btn = gr.Button(\"üîç Verify Fact\", variant=\"primary\")\n",
        "\n",
        "        # Output Section\n",
        "        with gr.Column(scale=2):\n",
        "            output = gr.Textbox(\n",
        "                label=\"Verification Result\",\n",
        "                lines=15\n",
        "            )\n",
        "\n",
        "    # Example inputs\n",
        "    gr.Examples(\n",
        "        examples=[\n",
        "            [\"The Indian government has announced free electricity to all farmers starting July 2025.\", 5],\n",
        "            [\"India achieved 100 crore COVID-19 vaccinations in October 2021.\", 5],\n",
        "            [\"GST was implemented in India in July 2017.\", 5],\n",
        "            [\"India's GDP growth is 20% in 2024.\", 5]\n",
        "        ],\n",
        "        inputs=[input_text, num_sources]\n",
        "    )\n",
        "\n",
        "    # Button -> Action\n",
        "    check_btn.click(\n",
        "        fn=check_fact_ui,\n",
        "        inputs=[input_text, num_sources],\n",
        "        outputs=output\n",
        "    )\n",
        "\n",
        "demo.launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "id": "tUBzHGIL081M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Thank you"
      ],
      "metadata": {
        "id": "O2OMOLUZ1BUK"
      }
    }
  ]
}